# -*- coding: utf-8 -*-
"""getVidComments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-8ATTMVpKQhhaLzxmxrYpuHg4NmxZFYv
"""

# import libraries 
from apiclient.discovery import build 
from urllib.parse import urlparse
from pathlib import Path
#done in google colab so using drive 
from google.colab import drive 
#mounting google drive to google colab 
drive.mount('/content/gdrive')
#importing OS to move between directorys in drive
import os
import time
import re
import pprint 
import json
import csv
import logging
import sys
import pandas as pd

# arguments to be passed to build function 
# Google Developer Key needed for YouTube Data API
DEVELOPER_KEY = "AIzaSyD3lX3yk9bWNNk1DcqKB6d-cMzLJsRhCPA" # pust your Google developer key here
#"AIzaSyAg2IZajtQ-GRF7AkIux2MMVAAk1eT_DKA"
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"

class MemoryCache():
    _CACHE = {}

    def get(self, url):
        return MemoryCache._CACHE.get(url)

    def set(self, url, content):
        MemoryCache._CACHE[url] = content

# creating youtube resource object 
# for interacting with API 
youtube = build(YOUTUBE_API_SERVICE_NAME,  
                     YOUTUBE_API_VERSION, 
            developerKey = DEVELOPER_KEY,
               cache=MemoryCache())


def getVideosIds(path):
    ids = []
    videos = pd.read_csv(path)
    for i, row in videos.iterrows():
        ids.append(get_video_id(row['vid_url']))
    return ids

def getCurrentDirectory():
    return os.getcwd()

def getParentDirectory():
    return str(Path(os.getcwd()).parent)

def get_video_id(the_link):
    vid = re.findall('v(?:=|%3D)([^&%?]+)', urlparse(the_link).query)
    if vid:
        return vid[0]
    the_path = urlparse(the_link).path[1:]
    if ((not the_path) or 
        the_path.startswith('user') or 
        the_path.startswith('shared') or
        the_path.startswith('playlist') or
        the_path.startswith('channel') or 
        the_path.startswith('attribution_link') or 
        the_path.startswith('results') or 
        the_path.startswith('edit') or 
        the_path.startswith('categories') or 
        the_path.startswith('c/') or 
        the_path.startswith('view_play_list')): 
            return None
    if the_path.startswith('embed/'): return the_path[len('embed/'):len('embed/')+11]#typical length of the id
    if the_path.startswith('v/'): return the_path[len('v/'):len('v/')+11]
    
    return the_path

def jsonExists(vid_id, path):
    files_names = [ name for name in os.listdir(path) if name.startswith(vid_id) and not os.path.isdir(os.path.join(path, name)) ]
    return len(files_names) > 0

#results_dir_path is the directory for saving output
#results_file_path is the csv file for inputting that has youtube videos
results_dir_path = '/content/gdrive/Shared drives/allComments/all'
results_file_path = '40-78.csv'

def main():
    #changes the OS to be in the drive
    os.chdir('/content/')
    
    #retrieves all of the video IDS from the CSV file of YouTube videos
    ids = getVideosIds(results_file_path)
    
    #changes the directory to the one for output
    os.chdir('/content/gdrive/Shared drives/allComments/all')
    List = []
    #for every video ID in the list of IDS
    for vid_id in ids:
        #clears list every iteration of for loop
        List.clear()
         # if json already fetched for youtube video then skip
        if jsonExists(vid_id, results_dir_path):
            continue
        
        try:
            #try to retrieve about 50-100 comments for a youTube video using the
            #YouTube data api and the video ID
            list_videos_byid = youtube.commentThreads().list( 
            part = "snippet, replies", videoId=vid_id, order = "time", textFormat="plainText"
                                               ).execute()
            #appends all of the comments to the List
            List.append(list_videos_byid.get("items", []))
            
            #if there are more comments to gather
            #use the nextPageToken and a while loop to keep going through
            #the video content until all the comments have been found
            while 'nextPageToken' in list_videos_byid:
                list_videos_byid = youtube.commentThreads().list( 
                part = "snippet, replies", videoId=vid_id, order = "time", textFormat="plainText", pageToken = list_videos_byid['nextPageToken']
                                               ).execute()
                #appends all of the comments to the List
                List.append(list_videos_byid.get("items", []))
        
        except:
            List.append("none")
        
        #checks to see if the list is empty
        if(List[0] != "none"):
            #prints out all the comments to json file
            with open(vid_id + '.json', 'w') as outfile:
                json.dump(obj=List, indent=4, sort_keys=True, fp = outfile)
        else:
            #prints out all the videos to a CSV file that weren't able to have their comments taken
            with open('unAvailable.csv', 'w') as outfile:
                writer = csv.writer(outfile)
                writer.writerow(["vid_id for unavailable vid"])
                for i in range(0, count):
                   writer.writerow(['https://www.youtube.com/watch?v=' + List[0]])
        
              
      

main()

print('Finished')

def printVideosDetails(results):
    # empty list to store video details 
    videos = [] 
      
    for result in results:
        details = f'''title: {result['snippet']['title']}
        \n tags: {result['snippet']['tags']}
        \n description: {result['snippet']['description']}
        \n publishedAt: {result['snippet']['publishedAt']}
        \n ChannelId: {result['snippet']['channelId']}
        \n ChannelTitle: {result['snippet']['channelTitle']}
        \n contentDetails: {result['contentDetails']}
        \n statistics: {result['statistics']}'''
        videos.append(details)
          
    print("Videos:\n", "\n".join(videos), "\n") 
printVideosDetails(results)

